rollout_type: ofa

## ---- Component search_space ----
# ---- Type ofa ----
search_space_type: ofa
search_space_cfg:
  num_cell_groups: [1, 4, 4, 4, 4, 4]
  expansions: [1, 6, 6, 6, 6, 6]
  schedule_cfg: {}
    # kernel_choice:
    #     type: value
    #     boundary: [1, 51, 120]
    #     value: [[7], [7, 5], [7, 5, 3]]
    # width_choice:
    #   type: value
    #   boundary: [1, 180, 240]
    #   value: [[6], [6,5,4], [6,5,4,3,2]]
    # depth_choice:
    #   type: value
    #   boundary: [1, 300, 360]
    #   value: [[4], [4, 3], [4,3,2]]
# ---- End Type mnsanet_ofa ----
## ---- End Component search_space ----

dataset_type: cifar10
dataset_cfg:
  # Schedulable attributes: 
  cutout: null


objective_type: ofa_classification
objective_cfg:
  # Schedulable attributes:
  schedule_cfg: {}
    # soft_loss_coeff:
    #   type: value
    #   boundary: [1, 60]
    #   value: [0.0, 0.0]
  soft_loss_coeff: 0.0
  latency_coeff: 1.0

final_model_type: ofa_final_model
final_model_cfg:
  backbone_type: mbv3_backbone
  genotypes: "cell_0=1, cell_1=2, cell_2=2, cell_3=3, cell_4=4, cell_5=3, cell_0_block_0=(1, 3), cell_1_block_0=(1, 3), cell_1_block_1=(4, 3), cell_1_block_2=(3, 5), cell_1_block_3=(2, 3), cell_2_block_0=(3, 3), cell_2_block_1=(3, 5), cell_2_block_2=(6, 3), cell_2_block_3=(5, 3), cell_3_block_0=(3, 5), cell_3_block_1=(3, 5), cell_3_block_2=(6, 3), cell_3_block_3=(3, 3), cell_4_block_0=(3, 3), cell_4_block_1=(2, 3), cell_4_block_2=(2, 3), cell_4_block_3=(6, 3), cell_5_block_0=(6, 3), cell_5_block_1=(6, 5), cell_5_block_2=(4, 5), cell_5_block_3=(4, 5)"
  backbone_cfg:
    mult_ratio: 1.0
    channels: [32, 16, 24, 40, 80, 112, 160, 960, 1280]
    # strides: [1, 2, 2, 2, 1, 2]
    strides: [1, 2, 1, 2, 1, 1]
    kernel_sizes: [3, 5, 7]
    stem_stride: 1
  ofa_state_dict: null

# final_trainer_type: ofa_final_trainer
# final_trainer_cfg:
#   batch_size: 128
#   epochs: 30
#   optimizer_scheduler:
#     eta_min: 0.00001
#     T_max: 30
#     type: CosineAnnealingLR
#   schedule_cfg: null
#   weight_decay: 0.0003
#   learning_rate: 0.0001
#   save_as_state_dict: true


final_trainer_type: cnn_trainer
final_trainer_cfg:
  # Schedulable attributes: 
  auxiliary_head: false
  auxiliary_weight: 0.0
  add_regularization: true
  batch_size: 128
  epochs: 100
  grad_clip: 5.0
  learning_rate: 0.1
  momentum: 0.9
  no_bias_decay: false
  optimizer_type: SGD
  optimizer_scheduler:
    type: MultiStepLR
    milestones: [40, 80]
    gamma: 0.1
  schedule_cfg: null
  warmup_epochs: 0
  weight_decay: 0.00004
  save_as_state_dict: true

